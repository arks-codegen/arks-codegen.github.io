<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="One embedder for all tasks">
  <meta name="keywords" content="Embedder, encoder">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>One Embedder, Any Task: Instruction-Finetuned Text Embeddings</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://hkunlp.github.io">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://yale-lily.github.io/spider">
            Spider
          </a>
          <a class="navbar-item" href="https://github.com/HKUNLP/UnifiedSKG">
            UnifiedSKG
          </a>
          <a class="navbar-item" href="https://github.com/Yushi-Hu/IC-DST">
            IC-DST
          </a>
          <a class="navbar-item" href="https://github.com/HKUNLP/icl-selective-annotation">
            Selective Annotation
          </a>
          <a class="navbar-item" href="https://ds1000-code-gen.github.io/">
            DS-1000
          </a>
          <a class="navbar-item" href="https://lm-code-binder.github.io/">
            Binder
          </a>
        </div>
      </div>
    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">One Embedder, Any Task: Instruction-Finetuned Text Embeddings</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://hongjin-su.github.io/">Hongjin Su*</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://weijia-shi.netlify.app/">Weijia Shi*</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="https://homes.cs.washington.edu/~jkasai/">Jungo Kasai</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="https://homes.cs.washington.edu/~yizhongw/">Yizhong Wang</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://yushi-hu.github.io/">Yushi Hu</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://people.ece.uw.edu/ostendorf/">Mari Ostendorf</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://scottyih.org/">Wen-tau Yih,</a><sup>3</sup>
            </span>
            <span class="author-block">
              <a href="https://nasmith.github.io/#:~:text=Noah%20Smith%20is%20a%20computer,a%20wide%20range%20of%20applications.">Noah A. Smith</a>,<sup>2,</sup><sup>4</sup>
            </span>
            <span class="author-block">
              <a href="https://www.cs.washington.edu/people/faculty/lsz">Luke Zettlemoyer</a>,<sup>2,</sup><sup>3</sup>
            </span>
            <span class="author-block">
              <a href="https://taoyds.github.io/">Tao Yu</a><sup>1</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>The University of Hong Kong,</span>
            <span class="author-block"><sup>2</sup>University of Washington,</span>
            <span class="author-block"><sup>3</sup>Meta AI,</span>
            <span class="author-block"><sup>4</sup>Allen Institute for AI</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2212.09741"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/HKUNLP/instructor-embedding"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Twitter Link. -->
              <span class="link-block">
                <a href="https://instructor-embedding.github.io/"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-twitter"></i>
                  </span>
                  <span>Twitter</span>
                  </a>
              </span>
              <span class="link-block">
                <a href="https://huggingface.co/hkunlp/instructor-large"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Checkpoint</span>
                  </a>
              </span>
              <span class="link-block">
                <a href="https://drive.google.com/file/d/1dvmDBp095CY5hwIJxcRNaLH7sIwym4ql/view"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-clone"></i>
                  </span>
                  <span>Data</span>
                  </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>
  
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <h2 class="subtitle has-text-centered">
        We introduce <span class="dnerf">Instructor</span>üë®‚Äçüè´, an instruction-finetuned text embedding model that can generate text embeddings tailored to any task (e.g., classification, retrieval, clustering, text evaluation, etc.) and domains (e.g., science, finance, etc.) by simply providing the task instruction, without any finetuning. Instructor achieves sota on 70 diverse embedding tasks!
      </h2>
      <p align="center">
        <img src="static/images/multitask.png" width="1000" height="500" class="center"> <!-- To .gif format-->
      </p>

    </div>
  </div>
</section>
  
 <section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            We introduce INSTRUCTOR, a new method for computing text embeddings given task instructions: every text input is embedded together with instructions explaining the use case (e.g., task and domain descriptions). Unlike encoders from prior work that are more specialized, INSTRUCTOR is a single embedder that can generate text embeddings tailored to different downstream tasks and domains, without any further training. We first annotate instructions for 330 diverse tasks and train INSTRUCTOR on this multitask mixture with a contrastive loss. We evaluate INSTRUCTOR on 70 embedding evaluation tasks (64 of which are unseen during training), ranging from classification and information retrieval to semantic textual similarity and text generation evaluation. INSTRUCTOR, while having an order of magnitude fewer parameters than the previous best model, achieves state-of-the-art performance, with an average improvement of 3.4% compared to the previous best results on the 70 diverse datasets. Our analysis suggests that INSTRUCTOR is robust to changes in instructions, and that instruction finetuning mitigates the challenge of training a single model on diverse datasets.
          </p>
          <span class="dnerf">INSTRUCTOR (335M)</span> is strong because:
        <ul>
          <li><strong>Efficient:</strong> &nbsp Fast adaptation!
            <br><span class="dnerf">INSTRUCTOR</span> can calculate domain-specific and task-aware embeddings without any further training. </li>
          <li><strong>General:</strong> &nbsp Any task!
            <br><span class="dnerf">INSTRUCTOR</span> can be applied to any task for computing fixed-length embeddings of texts. </li>
          <li><strong>Performance:</strong> &nbsp State-of-the-art!
            <br><span class="dnerf">INSTRUCTOR</span> achieves state-of-the-art performance on 70 datasets and surpasses models with an order of magnitude larger. </li>
        </ul>
        </div>
      </div>
    </div>
  </div>
    <!--/ Abstract. -->
  </div>
</section>
  
<section class="section">
  <div class="container is-max-desktop">
    <!-- Results. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Main Results</h2>
        <div class="content has-text-justified">
          <p>
            <span class="dnerf">INSTRUCTOR</span> achieves <strong>SOTA</strong> performance without further training on a wide range of tasks.
          </p>
        <ul>
          <li><a href="https://huggingface.co/spaces/mteb/leaderboard">MTEB</a> (56 diverse datasets including BEIR, STS, etc.)</li>
          <li><a href="https://arxiv.org/abs/2112.04139">Billboard</a> (Evaluate test generation quality)</li>
          <li><a href="https://arxiv.org/abs/2209.01975">Prompt Retrieval</a> (Retrieve examples for in-context learning) </li>
        </ul>
          <img src="static/images/main_results.png" class="center">
          <p>
            Retri., Pair., Class., Sum., Text Eval. refer to retrieval, pair classification, classification, summarization, and text evaluation, respectively. Compared to GTR(335M/1.5B), from which INSTRUCTOR (335M/1.5B) is initialized, instruction finetuninig enhances the performance by 5.9%. Compared to the state-of-the-art model (Sent-T5-XXL), INSTRUCTOR (335M/1.5B) achieves 3.4% and 4.1% performance gains respectively.
          </p>
        </div>
      </div>
    </div>
  </div>
    <!--/ Results. -->

</section>
  
<section class="section">
  <div class="container is-max-desktop">
    <!-- Results. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Analysis</h2>
        <div class="content has-text-justified">
          <p>
            Further, instruction has shown to be important to the INSTRUCTOR training on diverse data (Left), and super-NaturalInstruction datasets are critical to the INSTRUCTOR robustness of prompt paraphrase.
          </p>
        <img src="static/images/figure34.png" class="center">
          <p>
            In addition, by making instruction more detailed (Left) and model size larger (Right), the performance of INSTRUCTOR is consistently improved.
          </p>
        <img src="static/images/figure56.png" class="center">
          <p>
            With domain shift in evaluation, the improvement of instruction tuning is more pronounced.
          </p>
         <p align="center">
        <img src="static/images/domain_adaptation.png" width="400" height="100" class="center">
           </p>
          <p>
            Last but not least, we use T-SNE to visualize two examples. The green dot pair is an example with different sentiment. Without instructions, they are close in the embedding space (probably because they 4 same words in text). With instructions, INSTRUCTOR separates them apart and differentiate their sentiment. For red pairs, they are distant without instructions (probably because they look very different). With instructions, they are closer, which is aligned with their same emotion.
          </p>
        <p align="center">
           <img src="static/images/qualititive.png" width="400" height="400" class="center"> <!-- To .gif format-->
        </p>
        </div>
      </div>
    </div>
  </div>
    <!--/ Results. -->

</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">More instruction examples</h2>
        <div class="content has-text-justified">
        <p align="center">
           <img src="static/images/instruction_examples.png" class="center"> <!-- To .gif format-->
        </p>
        </div>
      </div>
    </div>
  </div>
    <!--/ Abstract. -->
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop content">
    <h2 class="title">Acknowledgement</h2>
   We thank <a href="https://akariasai.github.io/">Akari Asai</a>, Jack Lin, Minghan Li, and <a href="https://noahs-ark.github.io/">the ARK group at UW</a> for their helpful feedback on this work.
  </div>
</section>
 
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@inproceedings{INSTRUCTOR,
  title={One Embedder, Any Task: Instruction-Finetuned Text Embeddings},
  author={Hongjin Su, Weijia Shi, Jungo Kasai, Yizhong Wang, Yushi Hu, Mari Ostendorf, Wen-tau Yih, Noah A. Smith, Luke Zettlemoyer, Tao Yu},
  url={https://arxiv.org/abs/2212.09741},
  year={2022},
}</code></pre>
  </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
